---
layout: post
title:  "爬虫流程记录"
date:   2017-01-31
desc: "记录如何操作爬虫程序."
keywords: "Record process"
categories: [Basic]
tags: [流程记录]
icon: icon-html
---

1. [配置服务器](http://jingyan.baidu.com/article/f3ad7d0f129c7809c2345b56.html "server"):主要流程参考百度经验
2. [更改&上传代码](#upload)：
3. [爬取](#crawl)：
4. [检测](#check)：
5. [爬取结束](#crawl_end)：
	5. [检测数据](#check_data)：
	5. [抓取数据回本地或上传至百度云](#post_back)：
5. [再爬取](#reCrawl)：

----
<div id =""></div>
## 配置服务器
参考百度文献
<span id="upload"></span>
## 更改并上传代码
1. 主要是更改5个文件中的AutoSearch中的帐号为自己的信息，两个服务器各使用3个帐号（102-104,105-107）。
	* \ResearcherGoLinux\ResearcherGoAccounterTest\AccounterTestSpider\ResearcherGoAccounterTest\ResearcherGoAccounterTest\spiders\ResearchGoAutoSearch.py  
	* \ResearcherGoLinux\WOS_Spider
		* Spider_*\...\ResearchGoAutoSearch.py
2. 替换WOS_Scheduler/Jounral_list中的内容为自己的Journal_list，30个一批，末尾不能有空行
3. 上传代码至服务器的/tmp下（以后每爬取完成一次后用/tmp下的代码替换掉使用的代码）：windows命令行进入putty目录中并执行  
	pscp -r source_dir root@IP:/tmp
4. putty连接到服务器，复制代码到/home下  
	cp -r /tmp/source_dir /home  

<span id="crawl"></span>  

## 爬取
**转换换行符**：进入服务器中的Journal_list所在目录  
<code>sed -i "s/\r//" Journal_List.txt Journal_List_significance.txt</code>  
爬取开始前开启监控进程：  
	1. /home/ResearcherGoLinux/WOS_Monitor/ResearcherGoMonitor/ResercherGoMonitor.py  
	2. /home/ResearcherGoLinux/Progress_Monitor/ProgressMonitor/ProgressMonitor.py  
	3. 对上面两个文件赋予777权限并执行： 
<code>   
	chmod 777 target    
	python ./target
</code>  
查看本用户所在执行的进程：  
  
<code>
	ps -a  
	ps -a | grep -c scrapy #应当小于开启调度程序时的输出值（一般为1）  
	ps -a | grep -c python
</code>  

查看空间使用：  
<code>Top</code>

开始爬取：  
  1. 进入/home/ResearcherGoLinux/WOS_Scheduler/ResearcherGoScheduler/ 写777权限给ResearcherGoScheduler.py并执行  
  2. 输入最大scrapy数：1  
  3. 输入爬取期刊的起始与结束号  
  4. priority：General  
  5. input：Paper  
  6. input：year

<span id="crawl_end"></span>
## 爬取结束
*调度程序执行完成不一定表示已经爬取结束，需要查看是否还有scrapy进程在执行，若没有则代表爬取结束*：  
<code>  
	ps -a | grep -c scrapy
</code> 
 
<span id="check_data"></span>
### 检测数据
* 查看ScheduleData（已经调度的期刊）
* ProgressData（已经爬取的期刊）
* /WOS_Spider/ *General和New_Data是需要传回的数据文件夹*
	* /Data/General 
	* /Json_Data/New_Data  

<span id="post_back"></span>  
	
###  数据传回
  先将数据（General&New_Data）压缩到/tmp下,最好使用相对路径(先进入到General或New_Data所在的目录下):  
<code>  
	zip -r /tmp/xxx.zip General  
    zip -r /tmp/xxx.zip New_Data  
</code>
利用pscp将数据传回：  
<code>
	pscp root@IP:/tmp/source.zip target.zip 
</code>
利用百度云将数据传回：

<span id="reCrawl"></span>
### 再次爬取
  1. 再次爬取前将/home下的代码删除并从/tmp下重新复制过来. 将/tmp下的压缩数据也删除避免与后面的重复
<code>
	rm -r target  
	cp -r /tmp/target /home  
</code>
  2. 测试帐号： ResearcherGoLinux\ResearcherGoAccounterTest\AccounterController\AccounterCScheduler.py:
	赋权限并执行->input:Personal...
  3. 重复爬取流程


